{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0b1f5d-e6aa-4ee9-98b8-898e40569c61",
   "metadata": {},
   "source": [
    "## Continued Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e37487-f41a-4052-97db-2526d5cfc651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0.post200)\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0.post200\n",
      "    Uninstalling torch-2.0.0.post200:\n",
      "      Successfully uninstalled torch-2.0.0.post200\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.4.1 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires accelerate<0.22.0,>=0.21.0, but you have accelerate 0.34.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.1.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires transformers[sentencepiece]<4.41.0,>=4.36.0, but you have transformers 4.44.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.1.0 which is incompatible.\n",
      "sagemaker 2.227.0 requires boto3<2.0,>=1.34.142, but you have boto3 1.34.131 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: xformers<0.0.27 in /opt/conda/lib/python3.10/site-packages (0.0.22.post7)\n",
      "Collecting trl<0.9.0\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.3)\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trl\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.10.1\n",
      "    Uninstalling trl-0.10.1:\n",
      "      Successfully uninstalled trl-0.10.1\n",
      "Successfully installed trl-0.8.6\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: xformers in /opt/conda/lib/python3.10/site-packages (0.0.22.post7)\n",
      "Collecting xformers\n",
      "  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers) (1.26.4)\n",
      "Collecting torch==2.4.0 (from xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp310-cp310-linux_x86_64.whl (799.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.1/799.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->xformers) (12.1.105)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->xformers) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0->xformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0->xformers) (1.3.0)\n",
      "Installing collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, xformers\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.18.1\n",
      "    Uninstalling nvidia-nccl-cu12-2.18.1:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.18.1\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0\n",
      "    Uninstalling torch-2.1.0:\n",
      "      Successfully uninstalled torch-2.1.0\n",
      "  Attempting uninstall: xformers\n",
      "    Found existing installation: xformers 0.0.22.post7\n",
      "    Uninstalling xformers-0.0.22.post7:\n",
      "      Successfully uninstalled xformers-0.0.22.post7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires accelerate<0.22.0,>=0.21.0, but you have accelerate 0.34.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.4.0+cu121 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires transformers[sentencepiece]<4.41.0,>=4.36.0, but you have transformers 4.44.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.4.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.1.0.70 nvidia-nccl-cu12-2.20.5 torch-2.4.0+cu121 triton-3.0.0 xformers-0.0.27.post2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch\n",
    "import torch\n",
    "\n",
    "!pip install trl datasets -q\n",
    "\n",
    "!pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc141b3-94d3-4a5f-ab63-c816b24136b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 12:25:02.147549: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-10 12:25:02.147628: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-10 12:25:02.147656: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-10 12:25:02.331997: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, AutoConfig\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "030df311-f288-48f2-b2f3-d184df2b51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "            \"model_ckpt\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "            \"load_in_4bit\": True,\n",
    "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"use_lora\": True,\n",
    "            \"r\": 512,\n",
    "            \"lora_alpha\": 512,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"task_type\": \"CAUSAL_LM\",\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "            \"output_dir\": \"uae-law-pt\",\n",
    "            \"per_device_train_batch_size\": 1,\n",
    "            \"per_device_eval_batch_size\": 2,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"optim\": \"paged_adamw_32bit\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"save_strategy\": \"steps\",\n",
    "            \"logging_steps\": 2000,\n",
    "            \"num_train_epochs\": 5,\n",
    "            #\"max_steps\": 250,\n",
    "            \"fp16\": True,\n",
    "            \"push_to_hub\": False,\n",
    "            \"train_cln_name\": \"text\",\n",
    "            \"packing\": False,\n",
    "            \"max_seq_length\": 1024,\n",
    "            \"neftune_noise_alpha\": 5,\n",
    "            \"is_pretraining\": True\n",
    "        }\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c34910-f30f-4665-a03c-478e86efff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(train_config[\"model_ckpt\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "                                                    train_config[\"model_ckpt\"],\n",
    "                                                    vocab_size=len(tokenizer),\n",
    "                                                    n_ctx=train_config[\"max_seq_length\"],\n",
    "                                                    bos_token_id=tokenizer.bos_token_id,\n",
    "                                                    eos_token_id=tokenizer.eos_token_id,\n",
    "                                                )\n",
    "## Add the configurations for changing the number of layers, heads\n",
    "base_model = AutoModelForCausalLM.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea88079a-a0e7-41c5-a15b-fa758990f093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight True\n",
      "model.layers.0.self_attn.q_proj.weight True\n",
      "model.layers.0.self_attn.k_proj.weight True\n",
      "model.layers.0.self_attn.v_proj.weight True\n",
      "model.layers.0.self_attn.o_proj.weight True\n",
      "model.layers.0.mlp.gate_proj.weight True\n",
      "model.layers.0.mlp.up_proj.weight True\n",
      "model.layers.0.mlp.down_proj.weight True\n",
      "model.layers.0.input_layernorm.weight True\n",
      "model.layers.0.post_attention_layernorm.weight True\n",
      "model.layers.1.self_attn.q_proj.weight True\n",
      "model.layers.1.self_attn.k_proj.weight True\n",
      "model.layers.1.self_attn.v_proj.weight True\n",
      "model.layers.1.self_attn.o_proj.weight True\n",
      "model.layers.1.mlp.gate_proj.weight True\n",
      "model.layers.1.mlp.up_proj.weight True\n",
      "model.layers.1.mlp.down_proj.weight True\n",
      "model.layers.1.input_layernorm.weight True\n",
      "model.layers.1.post_attention_layernorm.weight True\n",
      "model.layers.2.self_attn.q_proj.weight True\n",
      "model.layers.2.self_attn.k_proj.weight True\n",
      "model.layers.2.self_attn.v_proj.weight True\n",
      "model.layers.2.self_attn.o_proj.weight True\n",
      "model.layers.2.mlp.gate_proj.weight True\n",
      "model.layers.2.mlp.up_proj.weight True\n",
      "model.layers.2.mlp.down_proj.weight True\n",
      "model.layers.2.input_layernorm.weight True\n",
      "model.layers.2.post_attention_layernorm.weight True\n",
      "model.layers.3.self_attn.q_proj.weight True\n",
      "model.layers.3.self_attn.k_proj.weight True\n",
      "model.layers.3.self_attn.v_proj.weight True\n",
      "model.layers.3.self_attn.o_proj.weight True\n",
      "model.layers.3.mlp.gate_proj.weight True\n",
      "model.layers.3.mlp.up_proj.weight True\n",
      "model.layers.3.mlp.down_proj.weight True\n",
      "model.layers.3.input_layernorm.weight True\n",
      "model.layers.3.post_attention_layernorm.weight True\n",
      "model.layers.4.self_attn.q_proj.weight True\n",
      "model.layers.4.self_attn.k_proj.weight True\n",
      "model.layers.4.self_attn.v_proj.weight True\n",
      "model.layers.4.self_attn.o_proj.weight True\n",
      "model.layers.4.mlp.gate_proj.weight True\n",
      "model.layers.4.mlp.up_proj.weight True\n",
      "model.layers.4.mlp.down_proj.weight True\n",
      "model.layers.4.input_layernorm.weight True\n",
      "model.layers.4.post_attention_layernorm.weight True\n",
      "model.layers.5.self_attn.q_proj.weight True\n",
      "model.layers.5.self_attn.k_proj.weight True\n",
      "model.layers.5.self_attn.v_proj.weight True\n",
      "model.layers.5.self_attn.o_proj.weight True\n",
      "model.layers.5.mlp.gate_proj.weight True\n",
      "model.layers.5.mlp.up_proj.weight True\n",
      "model.layers.5.mlp.down_proj.weight True\n",
      "model.layers.5.input_layernorm.weight True\n",
      "model.layers.5.post_attention_layernorm.weight True\n",
      "model.layers.6.self_attn.q_proj.weight True\n",
      "model.layers.6.self_attn.k_proj.weight True\n",
      "model.layers.6.self_attn.v_proj.weight True\n",
      "model.layers.6.self_attn.o_proj.weight True\n",
      "model.layers.6.mlp.gate_proj.weight True\n",
      "model.layers.6.mlp.up_proj.weight True\n",
      "model.layers.6.mlp.down_proj.weight True\n",
      "model.layers.6.input_layernorm.weight True\n",
      "model.layers.6.post_attention_layernorm.weight True\n",
      "model.layers.7.self_attn.q_proj.weight True\n",
      "model.layers.7.self_attn.k_proj.weight True\n",
      "model.layers.7.self_attn.v_proj.weight True\n",
      "model.layers.7.self_attn.o_proj.weight True\n",
      "model.layers.7.mlp.gate_proj.weight True\n",
      "model.layers.7.mlp.up_proj.weight True\n",
      "model.layers.7.mlp.down_proj.weight True\n",
      "model.layers.7.input_layernorm.weight True\n",
      "model.layers.7.post_attention_layernorm.weight True\n",
      "model.layers.8.self_attn.q_proj.weight True\n",
      "model.layers.8.self_attn.k_proj.weight True\n",
      "model.layers.8.self_attn.v_proj.weight True\n",
      "model.layers.8.self_attn.o_proj.weight True\n",
      "model.layers.8.mlp.gate_proj.weight True\n",
      "model.layers.8.mlp.up_proj.weight True\n",
      "model.layers.8.mlp.down_proj.weight True\n",
      "model.layers.8.input_layernorm.weight True\n",
      "model.layers.8.post_attention_layernorm.weight True\n",
      "model.layers.9.self_attn.q_proj.weight True\n",
      "model.layers.9.self_attn.k_proj.weight True\n",
      "model.layers.9.self_attn.v_proj.weight True\n",
      "model.layers.9.self_attn.o_proj.weight True\n",
      "model.layers.9.mlp.gate_proj.weight True\n",
      "model.layers.9.mlp.up_proj.weight True\n",
      "model.layers.9.mlp.down_proj.weight True\n",
      "model.layers.9.input_layernorm.weight True\n",
      "model.layers.9.post_attention_layernorm.weight True\n",
      "model.layers.10.self_attn.q_proj.weight True\n",
      "model.layers.10.self_attn.k_proj.weight True\n",
      "model.layers.10.self_attn.v_proj.weight True\n",
      "model.layers.10.self_attn.o_proj.weight True\n",
      "model.layers.10.mlp.gate_proj.weight True\n",
      "model.layers.10.mlp.up_proj.weight True\n",
      "model.layers.10.mlp.down_proj.weight True\n",
      "model.layers.10.input_layernorm.weight True\n",
      "model.layers.10.post_attention_layernorm.weight True\n",
      "model.layers.11.self_attn.q_proj.weight True\n",
      "model.layers.11.self_attn.k_proj.weight True\n",
      "model.layers.11.self_attn.v_proj.weight True\n",
      "model.layers.11.self_attn.o_proj.weight True\n",
      "model.layers.11.mlp.gate_proj.weight True\n",
      "model.layers.11.mlp.up_proj.weight True\n",
      "model.layers.11.mlp.down_proj.weight True\n",
      "model.layers.11.input_layernorm.weight True\n",
      "model.layers.11.post_attention_layernorm.weight True\n",
      "model.layers.12.self_attn.q_proj.weight True\n",
      "model.layers.12.self_attn.k_proj.weight True\n",
      "model.layers.12.self_attn.v_proj.weight True\n",
      "model.layers.12.self_attn.o_proj.weight True\n",
      "model.layers.12.mlp.gate_proj.weight True\n",
      "model.layers.12.mlp.up_proj.weight True\n",
      "model.layers.12.mlp.down_proj.weight True\n",
      "model.layers.12.input_layernorm.weight True\n",
      "model.layers.12.post_attention_layernorm.weight True\n",
      "model.layers.13.self_attn.q_proj.weight True\n",
      "model.layers.13.self_attn.k_proj.weight True\n",
      "model.layers.13.self_attn.v_proj.weight True\n",
      "model.layers.13.self_attn.o_proj.weight True\n",
      "model.layers.13.mlp.gate_proj.weight True\n",
      "model.layers.13.mlp.up_proj.weight True\n",
      "model.layers.13.mlp.down_proj.weight True\n",
      "model.layers.13.input_layernorm.weight True\n",
      "model.layers.13.post_attention_layernorm.weight True\n",
      "model.layers.14.self_attn.q_proj.weight True\n",
      "model.layers.14.self_attn.k_proj.weight True\n",
      "model.layers.14.self_attn.v_proj.weight True\n",
      "model.layers.14.self_attn.o_proj.weight True\n",
      "model.layers.14.mlp.gate_proj.weight True\n",
      "model.layers.14.mlp.up_proj.weight True\n",
      "model.layers.14.mlp.down_proj.weight True\n",
      "model.layers.14.input_layernorm.weight True\n",
      "model.layers.14.post_attention_layernorm.weight True\n",
      "model.layers.15.self_attn.q_proj.weight True\n",
      "model.layers.15.self_attn.k_proj.weight True\n",
      "model.layers.15.self_attn.v_proj.weight True\n",
      "model.layers.15.self_attn.o_proj.weight True\n",
      "model.layers.15.mlp.gate_proj.weight True\n",
      "model.layers.15.mlp.up_proj.weight True\n",
      "model.layers.15.mlp.down_proj.weight True\n",
      "model.layers.15.input_layernorm.weight True\n",
      "model.layers.15.post_attention_layernorm.weight True\n",
      "model.layers.16.self_attn.q_proj.weight True\n",
      "model.layers.16.self_attn.k_proj.weight True\n",
      "model.layers.16.self_attn.v_proj.weight True\n",
      "model.layers.16.self_attn.o_proj.weight True\n",
      "model.layers.16.mlp.gate_proj.weight True\n",
      "model.layers.16.mlp.up_proj.weight True\n",
      "model.layers.16.mlp.down_proj.weight True\n",
      "model.layers.16.input_layernorm.weight True\n",
      "model.layers.16.post_attention_layernorm.weight True\n",
      "model.layers.17.self_attn.q_proj.weight True\n",
      "model.layers.17.self_attn.k_proj.weight True\n",
      "model.layers.17.self_attn.v_proj.weight True\n",
      "model.layers.17.self_attn.o_proj.weight True\n",
      "model.layers.17.mlp.gate_proj.weight True\n",
      "model.layers.17.mlp.up_proj.weight True\n",
      "model.layers.17.mlp.down_proj.weight True\n",
      "model.layers.17.input_layernorm.weight True\n",
      "model.layers.17.post_attention_layernorm.weight True\n",
      "model.layers.18.self_attn.q_proj.weight True\n",
      "model.layers.18.self_attn.k_proj.weight True\n",
      "model.layers.18.self_attn.v_proj.weight True\n",
      "model.layers.18.self_attn.o_proj.weight True\n",
      "model.layers.18.mlp.gate_proj.weight True\n",
      "model.layers.18.mlp.up_proj.weight True\n",
      "model.layers.18.mlp.down_proj.weight True\n",
      "model.layers.18.input_layernorm.weight True\n",
      "model.layers.18.post_attention_layernorm.weight True\n",
      "model.layers.19.self_attn.q_proj.weight True\n",
      "model.layers.19.self_attn.k_proj.weight True\n",
      "model.layers.19.self_attn.v_proj.weight True\n",
      "model.layers.19.self_attn.o_proj.weight True\n",
      "model.layers.19.mlp.gate_proj.weight True\n",
      "model.layers.19.mlp.up_proj.weight True\n",
      "model.layers.19.mlp.down_proj.weight True\n",
      "model.layers.19.input_layernorm.weight True\n",
      "model.layers.19.post_attention_layernorm.weight True\n",
      "model.layers.20.self_attn.q_proj.weight True\n",
      "model.layers.20.self_attn.k_proj.weight True\n",
      "model.layers.20.self_attn.v_proj.weight True\n",
      "model.layers.20.self_attn.o_proj.weight True\n",
      "model.layers.20.mlp.gate_proj.weight True\n",
      "model.layers.20.mlp.up_proj.weight True\n",
      "model.layers.20.mlp.down_proj.weight True\n",
      "model.layers.20.input_layernorm.weight True\n",
      "model.layers.20.post_attention_layernorm.weight True\n",
      "model.layers.21.self_attn.q_proj.weight True\n",
      "model.layers.21.self_attn.k_proj.weight True\n",
      "model.layers.21.self_attn.v_proj.weight True\n",
      "model.layers.21.self_attn.o_proj.weight True\n",
      "model.layers.21.mlp.gate_proj.weight True\n",
      "model.layers.21.mlp.up_proj.weight True\n",
      "model.layers.21.mlp.down_proj.weight True\n",
      "model.layers.21.input_layernorm.weight True\n",
      "model.layers.21.post_attention_layernorm.weight True\n",
      "model.norm.weight True\n",
      "lm_head.weight True\n"
     ]
    }
   ],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "  print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1b8e4b-cada-4e7c-9dcb-e7b62180ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "                                    r=train_config[\"r\"],\n",
    "                                    lora_alpha=train_config[\"lora_alpha\"],\n",
    "                                    lora_dropout=train_config[\"lora_dropout\"],\n",
    "                                    bias=train_config[\"bias\"],\n",
    "                                    task_type=train_config[\"task_type\"],\n",
    "                                    target_modules=train_config[\"target_modules\"]\n",
    "                                )\n",
    "\n",
    "model = get_peft_model(base_model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c30dc13-a996-4a80-87c8-b1030acf3ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 403701760\n",
      "Total parameters: 1503750144\n",
      "Percentage of trainable parameters: 26.85%\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return trainable_params, total_params\n",
    "\n",
    "trainable_params, total_params = count_trainable_parameters(base_model)\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Percentage of trainable parameters: {(trainable_params / total_params) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb834f94-32ba-4b18-80ec-115efeda2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def formatting_func(context, question, answer):\n",
    "#     template = f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\n + \\\n",
    "#     You must output the SQL query that answers the question.\\n\\n + \\\n",
    "#     ### Input:\\n + \\\n",
    "#     ```{question}```\\n\\n + \\\n",
    "#     ### Context:\\n + \\\n",
    "#     ```{context}```\\n\\n + \\\n",
    "#     ### Response:\\n + \\\n",
    "#     ```{answer};```\"\"\"\n",
    "#     return template\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac6d68c5-bc9e-4280-8418-8da83eee42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_data():\n",
    "#     data = load_dataset(\"obadabaq/uae-laws\", split=\"train\")\n",
    "#     # data_df = data.to_pandas()\n",
    "#     # data_df = data_df[:5000]\n",
    "#     # data_df[\"text\"] = data_df[[\"input\", \"instruction\", \"output\"]].apply(lambda x: \"Human: \" + x[\"instruction\"] + \" \" + x[\"input\"] + \" Assistant: \"+ x[\"output\"], axis=1)\n",
    "#     # data = Dataset.from_pandas(data_df)\n",
    "#     return data\n",
    "\n",
    "# data = create_data()\n",
    "# data = format_for_pretraining(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4614ecf1-8623-4a2e-9c0c-3159eaca1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_data():\n",
    "#     # Load the dataset\n",
    "#     data = load_dataset(\"obadabaq/uae-laws\", split=\"train\")\n",
    "    \n",
    "#     # Format the dataset for pretraining\n",
    "#     def format_for_pretraining(examples):\n",
    "#         combined_text = f\"{examples['Law date & authority']} - {examples['Content']}\"\n",
    "#         return {\"text\": combined_text}\n",
    "    \n",
    "#     # Apply the formatting to the dataset\n",
    "#     data = data.map(format_for_pretraining, batched=False)\n",
    "\n",
    "#     split_data = data.train_test_split(test_size=0.1)\n",
    "    \n",
    "#     return split_data\n",
    "\n",
    "# data = create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ffd2b5-365e-448a-ad38-f2dc5d699fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44330bf36f440ea8fe537da13f24f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa21dd908654d988ff3f138ee33600f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8500, Test size: 945\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Function to create and split the dataset\n",
    "def create_data():\n",
    "    # Load the dataset and split into train and test sets (90% train, 10% test)\n",
    "    data = load_dataset(\"obadabaq/uae-laws\")\n",
    "    \n",
    "    # Split the data into train and test sets using Hugging Face's built-in split\n",
    "    split_data = data['train'].train_test_split(test_size=0.1)  # 90% train, 10% test\n",
    "    \n",
    "    # Format the dataset for pretraining\n",
    "    def format_for_pretraining(examples):\n",
    "        combined_text = f\"{examples['Law date & authority']} - {examples['Content']}\"\n",
    "        return {\"text\": combined_text}\n",
    "    \n",
    "    # Apply the formatting to both train and test sets\n",
    "    split_data['train'] = split_data['train'].map(format_for_pretraining, batched=False)\n",
    "    split_data['test'] = split_data['test'].map(format_for_pretraining, batched=False)\n",
    "    \n",
    "    return split_data\n",
    "\n",
    "# Create and split the data\n",
    "split_data = create_data()\n",
    "\n",
    "# Access the train and test sets\n",
    "train_data = split_data['train']\n",
    "test_data = split_data['test']\n",
    "\n",
    "# Verify the split\n",
    "print(f\"Train size: {len(train_data)}, Test size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a3c14a5-892f-4077-aa56-2acfe926fc9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Federal Decree Law No. (5) of 1985 Concerning the Issuance of the Civil Transactions Law of the United Arab Emirates - Article (71)  \\n(1)  The personality (status of person) of a human being shall commence at the moment of \\nbeing born alive.  It shall terminate upon his death.  \\n(2)  The law shall lay down the rights of a fetus in utero.  \\n  \\n \\n',\n",
       " 'Cabinet Resolution No. (8) of 2018 Concerning the Executive Regulations of Federal Law No. (1) of 2017 Concerning Anti-Dumping, Countervailing, and Preventive Measures - Article (27)  \\n1. The normal value shall be calculated based on the comparable price paid or payable, in the \\nordinary course of trade, for sales of similar product  by independent customers in the \\ndomestic market of the exporting country.  \\n2. Notwithstanding paragraph 1 above, where a product under  investigation is  not imported \\ndirectly from the country of origin but is exported to the State from an intermediate country, \\nthe normal value shall be established on the basis of comparable price paid or payable, in the \\nordinary course of trade, in the domestic market of the country of origin if the products are \\nnot produced in the exporting country (i.e., the products are merely transhipped  through the \\nexporting country), or there is no comparable price for them in the exporting country.  \\n3. In the case of an association, par tnerships agreements or a compensatory arrangement or \\nother related arrangements form of compensatory arrangement among Related Parties , prices \\namong them may be considered to be not in the ordinary course of trade and may not be used \\nto establish normal value.  \\n4. Sales of the similar product  destined for consumption in the domestic market of the exporting \\ncountry shall be considered to be of sufficient quantity for the determination of the normal \\nvalue if such sales constitute five percent (5%) or mor e of the export sales volume of the \\nproduct under investigation to the State. However, a volume of sales lower than five percent \\n(5%) of sales may be used if the Department is satisfied, based on the evidence to be submitted \\nor otherwise available, that th e sales of such lower volume are nonetheless of sufficient \\nmagnitude to provide for proper comparison.  Cabinet Resolution of 20 18 Concerning the Executive Regulations of Federal Law of 201 7 Concerning Anti -Dumping, \\nCountervailing, and Preventive Measures  18 \\n 5. When there are no sales of the similar product  in the ordinary course of trade in the domestic \\nmarket of the exporting country, or when a proper comparis on cannot be made under such \\nsales due to the particular market situation or the low volume of the sales in the domestic \\nmarket of the exporting country, the normal value of the similar product  shall be established \\non the basis of the cost of production in  the country of origin plus a reasonable amount for \\nadministrative selling and general costs as well as for profit margin, or on the basis  of export \\nprice, in the ordinary course of trade, to an appropriate  third country, provided that this price \\nis reason able.  \\n6. Sales of the similar product  in the domestic market of an exporting country or export sales to \\na third country at prices below per unit either fixed and variable, costs of production plus \\nadministration, selling and general costs shall not be treated  as being in the ordinary course of \\ntrade by reason of price, and; thereby, may be disregarded in determining the normal value, \\nonly if it is determined that such sales were made:  \\na. Within an extended period of time, which shall normally be for one (1) year and shall in \\nno case be less than six (6) months.  \\nb. In substantial  quantities, when it is established that the weighted average selling price of \\nthe transactions under consideration for the determination of the normal value is below \\nthe weighted average unit  cost, or that the volume of sales below cost is not less than \\ntwenty percent (20%)  of sales under consideration for the determination of the normal \\nvalue.  \\nc. At prices which do not provide for recovery of all costs within a reasonable period of time, \\nif pric es which are below per unit costs at the time of sale are above the weighted average \\nper unit cost for the period of investigation, such prices shall be considered as providing \\nfor recovery of costs within a reasonable period of time.  \\n7. Where the country exporting the product under investigation is a non -market economy \\ncountry, normal value may be determined on the basis of: - (1) the comparable price paid or \\npayable or constructed normal value, in the ordinary course of trade, for sales of the  simi lar \\nproduct  when destined for consumption in a market  economy of a third country; (2) the \\ncomparable price paid or payable, in the ordinary course of trade, for exports of the similar \\nproduct  from such a market economy of the third country to other countri es, including the Cabinet Resolution of 20 18 Concerning the Executive Regulations of Federal Law of 201 7 Concerning Anti -Dumping, \\nCountervailing, and Preventive Measures  19 \\n State; or (3) any other reasonable basis including the price actually  paid or payable in the State \\nfor the similar product , duly adjusted if necessary to include a reasonable profit margin.  \\n \\n',\n",
       " 'مرسوم بقانون اتحادي رقم (30) لسنة 2021 في شأن مكافحة المواد المخدرة والمؤثرات العقلية - املادة (8) \\n لوزير العدل باالتفاق مع الوزير املختص منح صفة مأموري الضبطالقضائ  لبعض موظف  الوزارات والهيئات  \\nوالدوائر ذات الصلة بتنفيذ أحكام اذا املرسوم بقانون،  ل في حدود اختصاصه.    \\n5 \\n مرسوم بقانون اتحادي في شأن مكافحة املواد املخدرة واملؤثرات العقلية  \\n ويكون لرؤساء الدوائر القضائية املحلية كالً في حدود اختصاصه منح صفة مأموري الضبط القضائ  ملوظف  \\nالجهات املحلية املعنية وفقًا للتشريعات السارية في اإل مارة. \\n \\n',\n",
       " 'Cabinet Resolution No. (45) of 2018 Concerning the Executive Regulations of Federal Law No. (11) of 2015 Controlling the Trade in and Stamping of Precious Stones and Metals - Article (7)  \\n1. The precious stones shall be accompanied by a certificate including the data \\nspecified in Annex No. (7) attached to this Resolution, according to the following:  \\na. One polished diamond piece if the weigh t of the one piece is equal to or \\nexceeding (0.30) carat.  \\nb. One piece of the precious stones (coloured) if the weight of the one piece is \\nequal to or exceeding (1) carat.  \\nc. One seed pearl if the measurement of the one seed id equal to or exceeding (3) \\nmm . \\n2. The  processed item shall be accompanied by a certificate if containing any of the \\nfollowing:  \\na. One or more polished diamond pieces if the weight of the one piece is equal to \\nor exceeding (0.5) carat.  \\nb. One piece or more of the precious stones (coloured) if the weight of the one \\npiece is equal to or exceeding (2) carats.  \\nc. One seed pearl or more if the measurement of the one seed is equal to or \\nexceeding (10.0) mm.  Cabinet Resolution of 20 18 Concerning the Executive Regulations of Federal Law of 201 5 Controlling \\nthe Trade in and Stamping of Precious Stones and Metals  7 \\n 3. Notwithstanding what had been contained in this Article, and dealer may request \\nthe certificate from  the certificates granting agency if the weights or measurements \\nare less than what had been specified in this Article.  \\n \\n',\n",
       " 'Federal Decree Law No. (5) of 1985 Concerning the Issuance of the Civil Transactions Law of the United Arab Emirates - Article (1300 ) \\n(1) The purchaser may bring an action against the pre -emptor before the court for an order \\neither that he exercise the right of pre- emption or that such right should lapse, and if the \\npre-emptor takes either of the aforesaid courses, he shall be bound by that course, and if he \\ndoes not make any election, the court shall terminate his right of pre- emption.  \\n(2) If the pre -emptor applies for extra time to consider whether to exercise the right or allow \\nit to lapse, the purchaser shall not be obliged to agree to such application.  \\n(3) A person who wishes to purchase may not make a claim against the pre- emptor either to \\nexercise or to waive his right prior to the purchase notwithstanding that he may have had \\nmade such a request prior thereto, and the pre- emptor has forfeited his right of pre- emption, \\nand in that event such forfeiture shall not be binding upon him.  \\n \\n',\n",
       " 'Federal Law No. (14) of 2020 on the Protection of Witnesses and the Like - Article (3) \\nApplicability to Protected Persons  \\nThe provisions of this Law shall apply to the following categories:  \\n1. Witness . \\n2. Victim . \\n3. Whistleblower . \\n4. Expert . \\n5. Anonymous source . \\nThe provisions of this Law shall also apply to the family members of the categories \\nmentioned in this Article, any person whose life or safety may be at risk due to his close \\nconnection to the protected person and any person[s] decided by the judicial auth ority to be \\ncovered with the protection.  \\n  \\n',\n",
       " \"Federal Decree by Law No. (9) of 2016 Concerning Bankruptcy - Article (187 ) \\n1. Notwithstanding the provisions of article (185) of this Decree -Law, if the trustee finds \\nthat the proceeds arising from the sale of any encumbered assets are not sufficient to \\ncover the trustee's fees and any costs relating to the sale of such assets, he may choose \\nnot to continue such sale, and t he trustee shall immediately notify the creditor holding  \\n \\nFederal Decree -Law No. (9) of 2016 concerning Bankruptcy  93 \\n the security in writing of any decision he may take in respect of failure to continue the \\nsale of the encumbered assets.  \\n2. Any creditor may object to the decision of the trustee within three (3) w orking days from \\nthe date of notification. The Court shall decide on such objection within five (5) working \\ndays without pleading. Its decision shall be final in this regard.  \\n  \\n\",\n",
       " 'Federal Decree by Law No. (38) of 2022, Promulgating the Criminal Procedures Law - Article (128) Service of Witnesses not inclu ded in the List  \\nEach litigant shall, through a process server at its own expense, summon his witnesses \\nwhose names are not included in the list prepared by the Public Prosecution.  \\n  \\n',\n",
       " \"Federal Decree by Law No. 32 of 2022 Concerning the Federal Judicial Authority - Article (77)  \\n1. The disciplinary action shall be filed bef ore the Disciplinary Board by the Head of the \\nJudicial Inspection Circuit or the Federal Attorney General, as the case may be, subject to \\nthe Council's approval, with a statement containing the violations and the evidence \\nsupporting them.  \\n2. The request to institute a disciplinary case must be preceded by an investigation \\nconducted by a Judge delegated by the Council for such purpose. The person who conducts \\nsuch investigation may not sit to decide on the disciplinary case.  \\n  \\n\",\n",
       " 'Federal Decree-Law No. (43) of 2021 On the Goods Subject to Non-Proliferation - Article (15)  \\nControls for Circulating Restricted and Hazardous Items  \\nWithout prejudice to the legislation in force in the State, the competent Entity shall issue a \\npermit for the circulation of the restricted items, which are listed in the Goods Schedule, \\nfrom the Weapons and Hazardous Substances Office, based upon the application submitted \\nby the licensing entity of the business, in accordance with the controls and procedures \\nspecified by the executive regulations of this Decree -Law.  \\n  \\n']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6b78018-1fce-4d5d-911a-25ea5b76bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "                                    do_eval=True,\n",
    "                                    eval_strategy = \"steps\",\n",
    "                                    eval_steps = 2000,\n",
    "                                    output_dir=train_config[\"output_dir\"],\n",
    "                                    per_device_train_batch_size=train_config[\"per_device_train_batch_size\"],\n",
    "                                    per_device_eval_batch_size=train_config[\"per_device_eval_batch_size\"],\n",
    "                                    gradient_accumulation_steps=train_config[\"gradient_accumulation_steps\"],\n",
    "                                    optim=train_config[\"optim\"],\n",
    "                                    learning_rate=train_config[\"learning_rate\"],\n",
    "                                    lr_scheduler_type=train_config[\"lr_scheduler_type\"],\n",
    "                                    save_strategy=train_config[\"save_strategy\"],\n",
    "                                    logging_steps=train_config[\"logging_steps\"],\n",
    "                                    num_train_epochs=train_config[\"num_train_epochs\"],\n",
    "                                    # max_steps=train_config[\"max_steps\"],\n",
    "                                    fp16=train_config[\"fp16\"],\n",
    "                                    push_to_hub=train_config[\"push_to_hub\"]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1e6d2f-2a17-4937-9f92-699b1ec02c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7451def9e8d44adca1b6c59d788bbe0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ffa75e8846450288091047f445815d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "                                    model=model,\n",
    "                                    train_dataset=train_data,\n",
    "                                    eval_dataset=test_data,\n",
    "                                    peft_config=lora_config,\n",
    "                                    dataset_text_field=train_config[\"train_cln_name\"],\n",
    "                                    args=args,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    packing=train_config[\"packing\"],\n",
    "                                    max_seq_length=train_config[\"max_seq_length\"]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de1acc-45b2-4ded-895a-2a139a8be62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8500' max='8500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8500/8500 41:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.241800</td>\n",
       "      <td>4.628575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.133600</td>\n",
       "      <td>4.069873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.754700</td>\n",
       "      <td>3.838519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.641900</td>\n",
       "      <td>3.767660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9494069c-dcf9-418a-9b29-18b7acbcd501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "output_dir = 'uae-law-pt/checkpoint-8500'\n",
    "#tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "local_model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d1e432b-6026-470e-8ce9-9fd13048c2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a chatbot that answers questions about the UAE law \n",
      "<|user|>\n",
      "What is the tax period for an incapacitated person ? \n",
      "<|assistant|>\n",
      "An incapacitated person refers to a person who is unable to make financial decisions or understand financial matters due to an incapacity or illness. As per the Income Tax Act (ITA), the tax period for an incapacitated person is six months. This period begins from the date of the incapacity or the date of the declaration of incapacity by the person. The tax period for incapacitated people is also known as the six-month tax period.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a chatbot that answers questions about the UAE law\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the tax period for an incapacitated person ?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = local_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe55fc67-926c-45d4-ad02-91ab4c0e30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "# Step 1: load the base model (Mistral-7B in our case) in 4-bit\n",
    "model_kwargs = dict(\n",
    "    # attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False,  # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",**model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f208cbb6-6f10-43a0-9f20-aa7cc509609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a chatbot that answers questions about the UAE law \n",
      "<|user|>\n",
      "What is the tax period for an incapacitated person ? \n",
      "<|assistant|>\n",
      "An incapacitated person refers to a person who is not able to manage their financial affairs or is unable to make decisions on their own behalf. The tax period for such a person is known as the tax period for the estate.\n",
      "\n",
      "The tax period for an incapacitated person is the time period between the date of the individual's incapacity and the date on which the individual dies. This period is known as the \"tax period for the estate.\" The tax period for the estate is the time period between the date of death and the date on which the estate is taxed.\n",
      "\n",
      "The tax period for an incapacitated person is usually longer than the tax period for the estate, as the incapacitated person may have a longer period during which to manage their affairs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a chatbot that answers questions about the UAE law\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the tax period for an incapacitated person ?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79d74576-bf44-4022-b2d4-c80fa81d0628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = PeftModel.from_pretrained(base_model, \"uae-law-pt/checkpoint-8500\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b8007c5-114b-4505-9ad6-9e73b0b58ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_merged = model.merge_and_unload().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db21f902-4ec9-4821-8d67-97b5276ad333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a chatbot that answers questions about the UAE law \n",
      "<|user|>\n",
      "What is the tax period for an incapacitated person ? \n",
      "<|assistant|>\n",
      "In the UAE, an incapacitated person's tax period is not defined in any law or regulation. This period is determined by the court or the relevant authority upon receiving a declaration from the person's lawyer or legal representative. The tax period for an incapacitated person generally begins from the day the declaration is issued, or the day of their incapacity, whichever comes first. After the expiry of the tax period, the person's tax liability is assessed and calculated based on their income during the period.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a chatbot that answers questions about the UAE law\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the tax period for an incapacitated person ?\"},\n",
    "]\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = mode_merged.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "179164e6-f29c-4486-b83e-036d474085bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354a9bffa325473d86b710f9df531248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02eba06c39544e9898c03787a3d235d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 6 LFS files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10075ef1237f4ff081a69de23f3d1b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8be4bebb5f44e2fb1262f62b69202f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a73023ef9f448db2685ccd2904c230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/3.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9996d24aa65541848ca7c9ce31647c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40081e00e90e46fd8f3e6a9cf8bef1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/GuhanTofu/TinyLlama-UAE-pt/commit/b0e340c83f9d5199729dc009d5ea5a0525500696', commit_message='Upload folder using huggingface_hub', commit_description='', oid='b0e340c83f9d5199729dc009d5ea5a0525500696', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'uae-law-pt/checkpoint-8500'\n",
    "model_name = 'TinyLlama-UAE-pt'\n",
    "username = 'GuhanTofu'\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi(token=\"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\")\n",
    "\n",
    "api.create_repo(\n",
    "    repo_id = f\"{username}/{model_name}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    repo_id = f\"{username}/{model_name}\",\n",
    "    folder_path = model_path\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7eed4f-8af3-426e-b494-15b2af3e41a9",
   "metadata": {},
   "source": [
    "## Supervised FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a693d4-47c6-418e-a1f8-7a0f1fac7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = {\n",
    "            \"model_ckpt\": \"GuhanTofu/TinyLlama-UAE-pt\",\n",
    "            \"load_in_4bit\": True,\n",
    "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"use_lora\": True,\n",
    "            \"r\": 512,\n",
    "            \"lora_alpha\": 512,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"task_type\": \"CAUSAL_LM\",\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "            \"output_dir\": \"\",\n",
    "            \"per_device_train_batch_size\": 1,\n",
    "            \"per_device_eval_batch_size\": 2,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"optim\": \"paged_adamw_32bit\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"save_strategy\": \"epoch\",\n",
    "            \"logging_steps\": 500,\n",
    "            \"num_train_epochs\": 1,\n",
    "            #\"max_steps\": 250,\n",
    "            \"fp16\": True,\n",
    "            \"push_to_hub\": ,\n",
    "            \"train_cln_name\": \"text\",\n",
    "            \"packing\": False,\n",
    "            \"max_seq_length\": 512,\n",
    "            \"neftune_noise_alpha\": 5,\n",
    "            \"is_pretraining\": False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b9a8d7-f11e-47f4-a7ae-cad9d61a459d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0554f1b35e7944e2a22c1f5bb14d646b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa394a51d1ce4d738b19ce69099156d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9406130a5f0b4d65876c396d35ab6f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f249df1b3c2d404899f0d2b9b7048cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aec381588164598bf37602a10b5ba0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/739 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad7c512c2fe42e0baed85daff3b89a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(sft_config[\"model_ckpt\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                sft_config[\"model_ckpt\"],\n",
    "                                load_in_4bit=sft_config[\"load_in_4bit\"],\n",
    "                                device_map=sft_config[\"device_map\"],\n",
    "                                torch_dtype=sft_config[\"torch_dtype\"]\n",
    "                            )\n",
    "model.config.use_cache=False\n",
    "model.config.pretraining_tp=1\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2805ca1-b3c0-4d7f-a369-bbc52b02f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "                                    r=sft_config[\"r\"],\n",
    "                                    lora_alpha=sft_config[\"lora_alpha\"],\n",
    "                                    lora_dropout=sft_config[\"lora_dropout\"],\n",
    "                                    bias=sft_config[\"bias\"],\n",
    "                                    task_type=sft_config[\"task_type\"],\n",
    "                                    target_modules=sft_config[\"target_modules\"]\n",
    "                                )\n",
    "\n",
    "model = get_peft_model(model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3743ce-fe8a-4f9d-a754-17d798bfe12d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.input_layernorm.weight False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.input_layernorm.weight False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.input_layernorm.weight False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.input_layernorm.weight False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.input_layernorm.weight False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.input_layernorm.weight False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.input_layernorm.weight False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.input_layernorm.weight False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.input_layernorm.weight False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.input_layernorm.weight False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.input_layernorm.weight False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.input_layernorm.weight False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.input_layernorm.weight False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.input_layernorm.weight False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.input_layernorm.weight False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.input_layernorm.weight False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.input_layernorm.weight False\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.input_layernorm.weight False\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.input_layernorm.weight False\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.input_layernorm.weight False\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.input_layernorm.weight False\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.input_layernorm.weight False\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight False\n",
      "base_model.model.model.norm.weight False\n",
      "base_model.model.lm_head.weight False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64bad5-f776-4551-9fec-93f108f40585",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"obadabaq/structured-uae-laws\", split=['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4857e1-2713-4f56-a5e7-4ae10aecfc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8f4c7-7ca6-4e4e-a451-4d53346eda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.train_test_split(test_size = 0.1)\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ebe2f-5479-4d87-bb41-d32d7cc60ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe609c-5111-48a4-9459-0d75c1d49cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "                                    do_eval=True,\n",
    "                                    output_dir=sft_config[\"output_dir\"],\n",
    "                                    per_device_train_batch_size=sft_config[\"per_device_train_batch_size\"],\n",
    "                                    gradient_accumulation_steps=sft_config[\"gradient_accumulation_steps\"],\n",
    "                                    optim=sft_config[\"optim\"],\n",
    "                                    learning_rate=sft_config[\"learning_rate\"],\n",
    "                                    lr_scheduler_type=sft_config[\"lr_scheduler_type\"],\n",
    "                                    save_strategy=sft_config[\"save_strategy\"],\n",
    "                                    logging_steps=sft_config[\"logging_steps\"],\n",
    "                                    num_train_epochs=sft_config[\"num_train_epochs\"],\n",
    "                                    # max_steps=sft_config[\"max_steps\"],\n",
    "                                    fp16=sft_config[\"fp16\"],\n",
    "                                    push_to_hub=sft_config[\"push_to_hub\"]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed5b20-c18e-4c04-883e-503e2a2c6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "                                    model=model,\n",
    "                                    train_dataset=dataset,\n",
    "                                    peft_config=lora_config,\n",
    "                                    dataset_text_field=sft_config[\"train_cln_name\"],\n",
    "                                    args=args,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    packing=sft_config[\"packing\"],\n",
    "                                    max_seq_length=sft_config[\"max_seq_length\"]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d31edc-84a7-4659-9b4d-2ed134ce8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9b8a3-82ff-4720-8296-2548a314941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "output_dir = ''\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe719fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=10,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "834079f3-c4b2-4786-aa94-71b081d86288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly UAE government assistant in UAE \n",
      "<|user|>\n",
      "What are the fines for misreporting Emiratisation percentages? \n",
      "<|assistant|>\n",
      "Fines for misreporting Emiratisation percentages vary depending on the extent of the fraud and the nature of the offense. For example:\n",
      "\n",
      "1. Emirati citizens involved in job recruitment: Fines may range from AED 5,000 to AED 50,000.\n",
      "\n",
      "2. Employers found to have falsely reported Emiratisation percentages: Fines may range from AED 5,000 to AED 50,000.\n",
      "\n",
      "Fines for misreporting Emiratisation percentages may include reduced government support, the suspension of Emiratisation certificates, and potential job losses.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What are the fines for misreporting Emiratisation percentages?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.85,\n",
    "        top_k=20,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb99e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "# Step 1: load the base model (Mistral-7B in our case) in 4-bit\n",
    "model_kwargs = dict(\n",
    "    # attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False,  # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",**model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a05bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly UAE government assistant in UAE \n",
      "<|user|>\n",
      "What is the fine for driving without insurance in Sharjah? \n",
      "<|assistant|>\n",
      "In Sharjah, driving without insurance is a punishable offense. The fine for driving without insurance in Sharjah is AED 300 ($90). The fine for the same offense in other Emirates and GCC countries is likely to be different. It is recommended to carry insurance while driving in Sharjah.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the fine for driving without insurance in Sharjah?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a0f0af2-a27f-45f2-abef-f3e073e52152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly UAE government assistant in UAE \n",
      "<|user|>\n",
      "What are the fines for misreporting Emiratisation percentages? \n",
      "<|assistant|>\n",
      "In the UAE, fines are imposed for misreporting Emiratisation percentages as per the UAE Labor Law. The maximum fine for violating this provision is AED 100,000 ($27,000 USD), and the fine is levied by the relevant authorities, including the UAE Ministry of Human Resources and Emiratization. The fines are imposed on individuals, organizations, and legal entities that violate the labor law\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What are the fines for misreporting Emiratisation percentages?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=10,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80bc974a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c28317aa5ee4bbe82f57290612ee2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d5e22b7c2d4de7a5d4569a3df3fc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/144M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c113093a254f30bcaf0849f02f61ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 6 LFS files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52c51e246ab440cbea8008c22b7f466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bf62a6555b4ed9990aa79f31951f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/72.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3aaf14478f348ca9b083a8858b8947d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42ff7ad629746f1865815892986fe47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/GuhanTofu/TinyLlama-UAE-sft/commit/e14bb2ba0a647bcb6aeaa01654014ad99682a6ad', commit_message='Upload folder using huggingface_hub', commit_description='', oid='e14bb2ba0a647bcb6aeaa01654014ad99682a6ad', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = 'sft-uae/checkpoint-1650'\n",
    "# model_name = 'TinyLlama-UAE-sft'\n",
    "# username = 'GuhanTofu'\n",
    "\n",
    "# from huggingface_hub import HfApi\n",
    "# api = HfApi(token=\"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\")\n",
    "\n",
    "# api.create_repo(\n",
    "#     repo_id = f\"{username}/{model_name}\",\n",
    "#     repo_type=\"model\"\n",
    "# )\n",
    "\n",
    "# api.upload_folder(\n",
    "#     repo_id = f\"{username}/{model_name}\",\n",
    "#     folder_path = model_path\n",
    "# )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29fe25a-9e3d-41cc-a21e-cfc008a5d060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef5406-f339-42de-8d74-163cf5fe348f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86e928dd",
   "metadata": {},
   "source": [
    "## Continued Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2c4e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 11:28:18.450118: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-04 11:28:18.450164: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-04 11:28:18.450183: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-04 11:28:18.455258: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, AutoConfig\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3192cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_config = {\n",
    "            \"model_ckpt\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "            \"load_in_4bit\": True,\n",
    "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"use_lora\": True,\n",
    "            \"r\": 128,\n",
    "            \"lora_alpha\": 128,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"task_type\": \"CAUSAL_LM\",\n",
    "            \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "            \"output_dir\": \"pt-uae\",\n",
    "            \"per_device_train_batch_size\": 1,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"optim\": \"paged_adamw_32bit\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"save_strategy\": \"epoch\",\n",
    "            \"logging_steps\": 100,\n",
    "            \"num_train_epochs\": 10,\n",
    "            #\"max_steps\": 250,\n",
    "            \"fp16\": True,\n",
    "            \"push_to_hub\": True,\n",
    "            \"train_cln_name\": \"text\",\n",
    "            \"packing\": False,\n",
    "            \"max_seq_length\": 512,\n",
    "            \"neftune_noise_alpha\": 5,\n",
    "            \"is_pretraining\": True\n",
    "        }\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f225d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(train_config[\"model_ckpt\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "                                                    train_config[\"model_ckpt\"],\n",
    "                                                    vocab_size=len(tokenizer),\n",
    "                                                    n_ctx=train_config[\"max_seq_length\"],\n",
    "                                                    bos_token_id=tokenizer.bos_token_id,\n",
    "                                                    eos_token_id=tokenizer.eos_token_id,\n",
    "                                                )\n",
    "## Add the configurations for changing the number of layers, heads\n",
    "model = AutoModelForCausalLM.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7f9432c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight True\n",
      "model.layers.0.self_attn.q_proj.weight True\n",
      "model.layers.0.self_attn.k_proj.weight True\n",
      "model.layers.0.self_attn.v_proj.weight True\n",
      "model.layers.0.self_attn.o_proj.weight True\n",
      "model.layers.0.mlp.gate_proj.weight True\n",
      "model.layers.0.mlp.up_proj.weight True\n",
      "model.layers.0.mlp.down_proj.weight True\n",
      "model.layers.0.input_layernorm.weight True\n",
      "model.layers.0.post_attention_layernorm.weight True\n",
      "model.layers.1.self_attn.q_proj.weight True\n",
      "model.layers.1.self_attn.k_proj.weight True\n",
      "model.layers.1.self_attn.v_proj.weight True\n",
      "model.layers.1.self_attn.o_proj.weight True\n",
      "model.layers.1.mlp.gate_proj.weight True\n",
      "model.layers.1.mlp.up_proj.weight True\n",
      "model.layers.1.mlp.down_proj.weight True\n",
      "model.layers.1.input_layernorm.weight True\n",
      "model.layers.1.post_attention_layernorm.weight True\n",
      "model.layers.2.self_attn.q_proj.weight True\n",
      "model.layers.2.self_attn.k_proj.weight True\n",
      "model.layers.2.self_attn.v_proj.weight True\n",
      "model.layers.2.self_attn.o_proj.weight True\n",
      "model.layers.2.mlp.gate_proj.weight True\n",
      "model.layers.2.mlp.up_proj.weight True\n",
      "model.layers.2.mlp.down_proj.weight True\n",
      "model.layers.2.input_layernorm.weight True\n",
      "model.layers.2.post_attention_layernorm.weight True\n",
      "model.layers.3.self_attn.q_proj.weight True\n",
      "model.layers.3.self_attn.k_proj.weight True\n",
      "model.layers.3.self_attn.v_proj.weight True\n",
      "model.layers.3.self_attn.o_proj.weight True\n",
      "model.layers.3.mlp.gate_proj.weight True\n",
      "model.layers.3.mlp.up_proj.weight True\n",
      "model.layers.3.mlp.down_proj.weight True\n",
      "model.layers.3.input_layernorm.weight True\n",
      "model.layers.3.post_attention_layernorm.weight True\n",
      "model.layers.4.self_attn.q_proj.weight True\n",
      "model.layers.4.self_attn.k_proj.weight True\n",
      "model.layers.4.self_attn.v_proj.weight True\n",
      "model.layers.4.self_attn.o_proj.weight True\n",
      "model.layers.4.mlp.gate_proj.weight True\n",
      "model.layers.4.mlp.up_proj.weight True\n",
      "model.layers.4.mlp.down_proj.weight True\n",
      "model.layers.4.input_layernorm.weight True\n",
      "model.layers.4.post_attention_layernorm.weight True\n",
      "model.layers.5.self_attn.q_proj.weight True\n",
      "model.layers.5.self_attn.k_proj.weight True\n",
      "model.layers.5.self_attn.v_proj.weight True\n",
      "model.layers.5.self_attn.o_proj.weight True\n",
      "model.layers.5.mlp.gate_proj.weight True\n",
      "model.layers.5.mlp.up_proj.weight True\n",
      "model.layers.5.mlp.down_proj.weight True\n",
      "model.layers.5.input_layernorm.weight True\n",
      "model.layers.5.post_attention_layernorm.weight True\n",
      "model.layers.6.self_attn.q_proj.weight True\n",
      "model.layers.6.self_attn.k_proj.weight True\n",
      "model.layers.6.self_attn.v_proj.weight True\n",
      "model.layers.6.self_attn.o_proj.weight True\n",
      "model.layers.6.mlp.gate_proj.weight True\n",
      "model.layers.6.mlp.up_proj.weight True\n",
      "model.layers.6.mlp.down_proj.weight True\n",
      "model.layers.6.input_layernorm.weight True\n",
      "model.layers.6.post_attention_layernorm.weight True\n",
      "model.layers.7.self_attn.q_proj.weight True\n",
      "model.layers.7.self_attn.k_proj.weight True\n",
      "model.layers.7.self_attn.v_proj.weight True\n",
      "model.layers.7.self_attn.o_proj.weight True\n",
      "model.layers.7.mlp.gate_proj.weight True\n",
      "model.layers.7.mlp.up_proj.weight True\n",
      "model.layers.7.mlp.down_proj.weight True\n",
      "model.layers.7.input_layernorm.weight True\n",
      "model.layers.7.post_attention_layernorm.weight True\n",
      "model.layers.8.self_attn.q_proj.weight True\n",
      "model.layers.8.self_attn.k_proj.weight True\n",
      "model.layers.8.self_attn.v_proj.weight True\n",
      "model.layers.8.self_attn.o_proj.weight True\n",
      "model.layers.8.mlp.gate_proj.weight True\n",
      "model.layers.8.mlp.up_proj.weight True\n",
      "model.layers.8.mlp.down_proj.weight True\n",
      "model.layers.8.input_layernorm.weight True\n",
      "model.layers.8.post_attention_layernorm.weight True\n",
      "model.layers.9.self_attn.q_proj.weight True\n",
      "model.layers.9.self_attn.k_proj.weight True\n",
      "model.layers.9.self_attn.v_proj.weight True\n",
      "model.layers.9.self_attn.o_proj.weight True\n",
      "model.layers.9.mlp.gate_proj.weight True\n",
      "model.layers.9.mlp.up_proj.weight True\n",
      "model.layers.9.mlp.down_proj.weight True\n",
      "model.layers.9.input_layernorm.weight True\n",
      "model.layers.9.post_attention_layernorm.weight True\n",
      "model.layers.10.self_attn.q_proj.weight True\n",
      "model.layers.10.self_attn.k_proj.weight True\n",
      "model.layers.10.self_attn.v_proj.weight True\n",
      "model.layers.10.self_attn.o_proj.weight True\n",
      "model.layers.10.mlp.gate_proj.weight True\n",
      "model.layers.10.mlp.up_proj.weight True\n",
      "model.layers.10.mlp.down_proj.weight True\n",
      "model.layers.10.input_layernorm.weight True\n",
      "model.layers.10.post_attention_layernorm.weight True\n",
      "model.layers.11.self_attn.q_proj.weight True\n",
      "model.layers.11.self_attn.k_proj.weight True\n",
      "model.layers.11.self_attn.v_proj.weight True\n",
      "model.layers.11.self_attn.o_proj.weight True\n",
      "model.layers.11.mlp.gate_proj.weight True\n",
      "model.layers.11.mlp.up_proj.weight True\n",
      "model.layers.11.mlp.down_proj.weight True\n",
      "model.layers.11.input_layernorm.weight True\n",
      "model.layers.11.post_attention_layernorm.weight True\n",
      "model.layers.12.self_attn.q_proj.weight True\n",
      "model.layers.12.self_attn.k_proj.weight True\n",
      "model.layers.12.self_attn.v_proj.weight True\n",
      "model.layers.12.self_attn.o_proj.weight True\n",
      "model.layers.12.mlp.gate_proj.weight True\n",
      "model.layers.12.mlp.up_proj.weight True\n",
      "model.layers.12.mlp.down_proj.weight True\n",
      "model.layers.12.input_layernorm.weight True\n",
      "model.layers.12.post_attention_layernorm.weight True\n",
      "model.layers.13.self_attn.q_proj.weight True\n",
      "model.layers.13.self_attn.k_proj.weight True\n",
      "model.layers.13.self_attn.v_proj.weight True\n",
      "model.layers.13.self_attn.o_proj.weight True\n",
      "model.layers.13.mlp.gate_proj.weight True\n",
      "model.layers.13.mlp.up_proj.weight True\n",
      "model.layers.13.mlp.down_proj.weight True\n",
      "model.layers.13.input_layernorm.weight True\n",
      "model.layers.13.post_attention_layernorm.weight True\n",
      "model.layers.14.self_attn.q_proj.weight True\n",
      "model.layers.14.self_attn.k_proj.weight True\n",
      "model.layers.14.self_attn.v_proj.weight True\n",
      "model.layers.14.self_attn.o_proj.weight True\n",
      "model.layers.14.mlp.gate_proj.weight True\n",
      "model.layers.14.mlp.up_proj.weight True\n",
      "model.layers.14.mlp.down_proj.weight True\n",
      "model.layers.14.input_layernorm.weight True\n",
      "model.layers.14.post_attention_layernorm.weight True\n",
      "model.layers.15.self_attn.q_proj.weight True\n",
      "model.layers.15.self_attn.k_proj.weight True\n",
      "model.layers.15.self_attn.v_proj.weight True\n",
      "model.layers.15.self_attn.o_proj.weight True\n",
      "model.layers.15.mlp.gate_proj.weight True\n",
      "model.layers.15.mlp.up_proj.weight True\n",
      "model.layers.15.mlp.down_proj.weight True\n",
      "model.layers.15.input_layernorm.weight True\n",
      "model.layers.15.post_attention_layernorm.weight True\n",
      "model.layers.16.self_attn.q_proj.weight True\n",
      "model.layers.16.self_attn.k_proj.weight True\n",
      "model.layers.16.self_attn.v_proj.weight True\n",
      "model.layers.16.self_attn.o_proj.weight True\n",
      "model.layers.16.mlp.gate_proj.weight True\n",
      "model.layers.16.mlp.up_proj.weight True\n",
      "model.layers.16.mlp.down_proj.weight True\n",
      "model.layers.16.input_layernorm.weight True\n",
      "model.layers.16.post_attention_layernorm.weight True\n",
      "model.layers.17.self_attn.q_proj.weight True\n",
      "model.layers.17.self_attn.k_proj.weight True\n",
      "model.layers.17.self_attn.v_proj.weight True\n",
      "model.layers.17.self_attn.o_proj.weight True\n",
      "model.layers.17.mlp.gate_proj.weight True\n",
      "model.layers.17.mlp.up_proj.weight True\n",
      "model.layers.17.mlp.down_proj.weight True\n",
      "model.layers.17.input_layernorm.weight True\n",
      "model.layers.17.post_attention_layernorm.weight True\n",
      "model.layers.18.self_attn.q_proj.weight True\n",
      "model.layers.18.self_attn.k_proj.weight True\n",
      "model.layers.18.self_attn.v_proj.weight True\n",
      "model.layers.18.self_attn.o_proj.weight True\n",
      "model.layers.18.mlp.gate_proj.weight True\n",
      "model.layers.18.mlp.up_proj.weight True\n",
      "model.layers.18.mlp.down_proj.weight True\n",
      "model.layers.18.input_layernorm.weight True\n",
      "model.layers.18.post_attention_layernorm.weight True\n",
      "model.layers.19.self_attn.q_proj.weight True\n",
      "model.layers.19.self_attn.k_proj.weight True\n",
      "model.layers.19.self_attn.v_proj.weight True\n",
      "model.layers.19.self_attn.o_proj.weight True\n",
      "model.layers.19.mlp.gate_proj.weight True\n",
      "model.layers.19.mlp.up_proj.weight True\n",
      "model.layers.19.mlp.down_proj.weight True\n",
      "model.layers.19.input_layernorm.weight True\n",
      "model.layers.19.post_attention_layernorm.weight True\n",
      "model.layers.20.self_attn.q_proj.weight True\n",
      "model.layers.20.self_attn.k_proj.weight True\n",
      "model.layers.20.self_attn.v_proj.weight True\n",
      "model.layers.20.self_attn.o_proj.weight True\n",
      "model.layers.20.mlp.gate_proj.weight True\n",
      "model.layers.20.mlp.up_proj.weight True\n",
      "model.layers.20.mlp.down_proj.weight True\n",
      "model.layers.20.input_layernorm.weight True\n",
      "model.layers.20.post_attention_layernorm.weight True\n",
      "model.layers.21.self_attn.q_proj.weight True\n",
      "model.layers.21.self_attn.k_proj.weight True\n",
      "model.layers.21.self_attn.v_proj.weight True\n",
      "model.layers.21.self_attn.o_proj.weight True\n",
      "model.layers.21.mlp.gate_proj.weight True\n",
      "model.layers.21.mlp.up_proj.weight True\n",
      "model.layers.21.mlp.down_proj.weight True\n",
      "model.layers.21.input_layernorm.weight True\n",
      "model.layers.21.post_attention_layernorm.weight True\n",
      "model.norm.weight True\n",
      "lm_head.weight True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfaa5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "                                    r=train_config[\"r\"],\n",
    "                                    lora_alpha=train_config[\"lora_alpha\"],\n",
    "                                    lora_dropout=train_config[\"lora_dropout\"],\n",
    "                                    bias=train_config[\"bias\"],\n",
    "                                    task_type=train_config[\"task_type\"],\n",
    "                                    target_modules=train_config[\"target_modules\"]\n",
    "                                )\n",
    "\n",
    "model = get_peft_model(model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130f759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def formatting_func(context, question, answer):\n",
    "#     template = f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\n + \\\n",
    "#     You must output the SQL query that answers the question.\\n\\n + \\\n",
    "#     ### Input:\\n + \\\n",
    "#     ```{question}```\\n\\n + \\\n",
    "#     ### Context:\\n + \\\n",
    "#     ```{context}```\\n\\n + \\\n",
    "#     ### Response:\\n + \\\n",
    "#     ```{answer};```\"\"\"\n",
    "#     return template\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e0d9193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    data = load_dataset(\"GuhanTofu/RE_UAE_regulations\", split=\"train\")\n",
    "    # data_df = data.to_pandas()\n",
    "    # data_df = data_df[:5000]\n",
    "    # data_df[\"text\"] = data_df[[\"input\", \"instruction\", \"output\"]].apply(lambda x: \"Human: \" + x[\"instruction\"] + \" \" + x[\"input\"] + \" Assistant: \"+ x[\"output\"], axis=1)\n",
    "    # data = Dataset.from_pandas(data_df)\n",
    "    return data\n",
    "\n",
    "data = create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ce1d85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '2019 Issue Year3 2\\nMission\\nTo create an innovative and\\nsustainable real estate\\nenvironment that shall promote \\nDubai the World’s happiest city \\nthrough:\\n• Smart services.\\n• Professional human and financial\\n   resources.\\n• Integrated Real Estate Legislations\\n   of Dubai.Vision\\nTo Position Dubai as the \\nWorld’s premier real estate \\ndestination, and a byword \\nfor innovation, trust and \\nhappiness.5 4INDEX\\nLaw No. ( 6) of 2019  Concerning\\nOwnership of Jointly Owned Real Property in the Emirate of Dubai\\n...................................................................................................................................................................................\\nLaw No. ( 4) of 2019  Concerning\\nThe Real Estate Regulatory Agency\\n...................................................................................................................................................................................\\nDecree No. ( 31) of 2016  Concerning\\nThe Mortgage of Granted Land in the Emirate of Dubai\\n...................................................................................................................................................................................\\nExecutive Council Resolution No. ( 37) of 2015 Regulating the Real Property\\nValuation Profession in the Emirate of Dubai\\n..................................................................................................................................................................................\\nLaw No. ( 7) of 2013\\nConcerning the Land Department\\n..................................................................................................................................................................................\\nDecree No. ( 43) of 2013  Determining\\nRent Increase for Real Property in the Emirate of Dubai\\n..................................................................................................................................................................................\\nDecree No. ( 26) of 2013  Concerning\\nThe Rental Disputes Settlement Centre in the Emirate of Dubai\\n..................................................................................................................................................................................\\nExecutive Council Resolution No. ( 30) of 2013\\nApproving Fees of the Land Department\\n .................................................................................................................................................................................\\nDecree No. ( 4) of 2010  Regulating the Transfer of Title to\\nGranted Industrial and Commercial Land in the Emirate of Dubai\\n.................................................................................................................................................................................\\nDecree No. ( 56) of 2009  Forming a Special Tribunal to Determine\\nCheque Disputes Relating to Real Property Transactions\\n...............................'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40193149",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "                                    do_eval=True,\n",
    "                                    output_dir=train_config[\"output_dir\"],\n",
    "                                    per_device_train_batch_size=train_config[\"per_device_train_batch_size\"],\n",
    "                                    gradient_accumulation_steps=train_config[\"gradient_accumulation_steps\"],\n",
    "                                    optim=train_config[\"optim\"],\n",
    "                                    learning_rate=train_config[\"learning_rate\"],\n",
    "                                    lr_scheduler_type=train_config[\"lr_scheduler_type\"],\n",
    "                                    save_strategy=train_config[\"save_strategy\"],\n",
    "                                    logging_steps=train_config[\"logging_steps\"],\n",
    "                                    num_train_epochs=train_config[\"num_train_epochs\"],\n",
    "                                    # max_steps=train_config[\"max_steps\"],\n",
    "                                    fp16=train_config[\"fp16\"],\n",
    "                                    push_to_hub=train_config[\"push_to_hub\"]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d80ae928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "                                    model=model,\n",
    "                                    train_dataset=data,\n",
    "                                    #peft_config=lora_config,\n",
    "                                    dataset_text_field=train_config[\"train_cln_name\"],\n",
    "                                    args=args,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    packing=train_config[\"packing\"],\n",
    "                                    max_seq_length=train_config[\"max_seq_length\"]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafada3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='212' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 212/3340 01:22 < 20:24, 2.55 it/s, Epoch 0.63/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.270800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef508498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "output_dir = 'pt-uae/checkpoint-3340'\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c90155e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly UAE government assistant in UAE \n",
      "<|user|>\n",
      "The DLD must give the purchaser a grace period of how much days ? \n",
      "<|assistant|>\n",
      "The DLD must give the purchaser a grace period of how many days from the date of delivery.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"The DLD must give the purchaser a grace period of how much days ?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321b49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "# Step 1: load the base model (Mistral-7B in our case) in 4-bit\n",
    "model_kwargs = dict(\n",
    "    # attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False,  # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",**model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec9e50c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly UAE government assistant in UAE \n",
      "<|user|>\n",
      "What is the Brokerage Agreement ? \n",
      "<|assistant|>\n",
      "The Brokerage Agreement is a legal agreement that governs the relationship between a broker and their client. It outlines the terms and conditions of the broker's services, including the fees charged, the scope of services provided, and the responsibilities of both parties. The broker's services typically include finding and negotiating deals for clients, managing the execution of transactions, and providing other related services. The brokerage agreement is an essential document for brokerage firms, as it establishes the legal obligations and responsibilities of both parties.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the Brokerage Agreement ?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "109207bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "model = PeftModel.from_pretrained(base_model, \"pt-uae/checkpoint-3340\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c4f5b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly UAE government assistant in UAE, use the information you have been pretrained on to answer \n",
      "<|user|>\n",
      "What is the Brokerage Agreement ? \n",
      "<|assistant|>\n",
      "The Brokerage Agreement is a legal contract between two parties, in this case, a broker and an investor, that outlines the terms and conditions under which the broker will sell or purchase securities on behalf of the investor. The broker agrees to sell the investor's securities and to provide the investor with the brokerage services, including transaction processing, marketing, and sales. The broker also agrees to take on the risk of market fluctuations and to provide the investor with access to other financial products, such as bonds or loans. The broker may also offer other services, such as investment banking or asset management, to the investor.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE, use the information you have been pretrained on to answer\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the Brokerage Agreement ?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4836516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8ad0835d8a470390683d7ae9ed0a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 6 LFS files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d754e257da14128ba90d849e96ead04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b2934439244d509cfed654ddf4ae1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4dc1899c1b45e0aacfb6ee79ca1059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/72.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3428fe93284809864259dfeceb0efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/144M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc899b15b4f4b9d83ad0c5e591f6e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa47ff36b1074a32a4aac7becbdac1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/GuhanTofu/TinyLlama-UAE-pt/commit/72c68b22da6e2bf5f7d67b1b5bd7c11614b4cc01', commit_message='Upload folder using huggingface_hub', commit_description='', oid='72c68b22da6e2bf5f7d67b1b5bd7c11614b4cc01', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = 'pt-uae/checkpoint-200'\n",
    "# model_name = 'TinyLlama-UAE-pt'\n",
    "# username = 'GuhanTofu'\n",
    "\n",
    "# from huggingface_hub import HfApi\n",
    "# api = HfApi(token=\"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\")\n",
    "\n",
    "# api.create_repo(\n",
    "#     repo_id = f\"{username}/{model_name}\",\n",
    "#     repo_type=\"model\"\n",
    "# )\n",
    "\n",
    "# api.upload_folder(\n",
    "#     repo_id = f\"{username}/{model_name}\",\n",
    "#     folder_path = model_path\n",
    "# )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928b7a7",
   "metadata": {},
   "source": [
    "## Supervised FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de331908",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = {\n",
    "            \"model_ckpt\": \"GuhanTofu/TinyLlama-UAE-pt\",\n",
    "            \"load_in_4bit\": True,\n",
    "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"use_lora\": True,\n",
    "            \"r\": 128,\n",
    "            \"lora_alpha\": 128,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"task_type\": \"CAUSAL_LM\",\n",
    "            \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "            \"output_dir\": \"sft-uae\",\n",
    "            \"per_device_train_batch_size\": 1,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"optim\": \"paged_adamw_32bit\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"save_strategy\": \"epoch\",\n",
    "            \"logging_steps\": 100,\n",
    "            \"num_train_epochs\": 10,\n",
    "            #\"max_steps\": 250,\n",
    "            \"fp16\": True,\n",
    "            \"push_to_hub\": True,\n",
    "            \"train_cln_name\": \"text\",\n",
    "            \"packing\": False,\n",
    "            \"max_seq_length\": 512,\n",
    "            \"neftune_noise_alpha\": 5,\n",
    "            \"is_pretraining\": False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cc014e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cab7b7cf65b427082f3764ad9afeef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b2a4c817d349f19abd7090f7d102eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41c3cc158eb494f9e7ace2e644f91cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dcf44c84fe4644ac7b64486b4589f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(sft_config[\"model_ckpt\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                sft_config[\"model_ckpt\"],\n",
    "                                load_in_4bit=sft_config[\"load_in_4bit\"],\n",
    "                                device_map=sft_config[\"device_map\"],\n",
    "                                torch_dtype=sft_config[\"torch_dtype\"]\n",
    "                            )\n",
    "model.config.use_cache=False\n",
    "model.config.pretraining_tp=1\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a5d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "                                    r=sft_config[\"r\"],\n",
    "                                    lora_alpha=sft_config[\"lora_alpha\"],\n",
    "                                    lora_dropout=sft_config[\"lora_dropout\"],\n",
    "                                    bias=sft_config[\"bias\"],\n",
    "                                    task_type=sft_config[\"task_type\"],\n",
    "                                    target_modules=sft_config[\"target_modules\"]\n",
    "                                )\n",
    "\n",
    "model = get_peft_model(model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3e3631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.0.input_layernorm.weight False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.1.input_layernorm.weight False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.2.input_layernorm.weight False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.3.input_layernorm.weight False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.4.input_layernorm.weight False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.5.input_layernorm.weight False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.6.input_layernorm.weight False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.7.input_layernorm.weight False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.8.input_layernorm.weight False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.9.input_layernorm.weight False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.10.input_layernorm.weight False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.11.input_layernorm.weight False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.12.input_layernorm.weight False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.13.input_layernorm.weight False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.14.input_layernorm.weight False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.15.input_layernorm.weight False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.16.input_layernorm.weight False\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.17.input_layernorm.weight False\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.18.input_layernorm.weight False\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.19.input_layernorm.weight False\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.20.input_layernorm.weight False\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.21.input_layernorm.weight False\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight False\n",
      "base_model.model.model.norm.weight False\n",
      "base_model.model.lm_head.weight False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce749692",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"GuhanTofu/sft-UAE_rules\", split=['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c085f829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['prompt', 'response'],\n",
       "     num_rows: 165\n",
       " })]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cb270c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an input that requires about the law in UAE, Write a response that appropriately completes the request.\n",
    "\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs       = examples[\"prompt\"]\n",
    "    outputs      = examples[\"response\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"GuhanTofu/sft-UAE_rules\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "515f3b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'text'],\n",
       "    num_rows: 165\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc3e957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "                                    do_eval=True,\n",
    "                                    output_dir=sft_config[\"output_dir\"],\n",
    "                                    per_device_train_batch_size=sft_config[\"per_device_train_batch_size\"],\n",
    "                                    gradient_accumulation_steps=sft_config[\"gradient_accumulation_steps\"],\n",
    "                                    optim=sft_config[\"optim\"],\n",
    "                                    learning_rate=sft_config[\"learning_rate\"],\n",
    "                                    lr_scheduler_type=sft_config[\"lr_scheduler_type\"],\n",
    "                                    save_strategy=sft_config[\"save_strategy\"],\n",
    "                                    logging_steps=sft_config[\"logging_steps\"],\n",
    "                                    num_train_epochs=sft_config[\"num_train_epochs\"],\n",
    "                                    # max_steps=sft_config[\"max_steps\"],\n",
    "                                    fp16=sft_config[\"fp16\"],\n",
    "                                    push_to_hub=sft_config[\"push_to_hub\"]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "325744c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6dbd9da9474ccfa83b9b1c751bf85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "                                    model=model,\n",
    "                                    train_dataset=dataset,\n",
    "                                    peft_config=lora_config,\n",
    "                                    dataset_text_field=sft_config[\"train_cln_name\"],\n",
    "                                    args=args,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    packing=sft_config[\"packing\"],\n",
    "                                    max_seq_length=sft_config[\"max_seq_length\"]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b2689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='641' max='1650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 641/1650 02:08 < 03:22, 4.97 it/s, Epoch 3.88/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.778500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.842200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.788200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.751900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c68926e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "output_dir = 'sft-uae/checkpoint-1650'\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f2c34c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly UAE government assistant in UAE \n",
      "<|user|>\n",
      "Can legal action be taken alongside fines under 'Nafis'? \n",
      "<|assistant|>\n",
      "Yes, legal action may also be taken alongside fines under 'Nafis' in response to cases of fraud or misconduct. The Ministry of Finance may initiate legal proceedings against applicants or beneficiaries who have fraudulently obtained support, or who have misrepresented their circumstances in order to receive funding. The aim is to protect the interests of the Emirati community and prevent further misuse of government programs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Can legal action be taken alongside fines under 'Nafis'?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=10,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80eb78c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly UAE government assistant in UAE \n",
      "<|user|>\n",
      "What are the fines for misreporting Emiratisation percentages? \n",
      "<|assistant|>\n",
      "Fines for misreporting Emiratisation percentages vary depending on the extent of the fraud and the nature of the offense. For example:\n",
      "\n",
      "1. Emirati citizens involved in job recruitment: Fines may range from AED 5,000 to AED 50,000.\n",
      "\n",
      "2. Employers found to have falsely reported Emiratisation percentages: Fines may range from AED 5,000 to AED 50,000.\n",
      "\n",
      "Fines for misreporting Emiratisation percentages may include reduced government support, the suspension of Emiratisation certificates, and potential job losses.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What are the fines for misreporting Emiratisation percentages?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.85,\n",
    "        top_k=20,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028b80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "# Step 1: load the base model (Mistral-7B in our case) in 4-bit\n",
    "model_kwargs = dict(\n",
    "    # attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False,  # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",**model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a875fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly UAE government assistant in UAE \n",
      "<|user|>\n",
      "What is the fine for driving without insurance in Sharjah? \n",
      "<|assistant|>\n",
      "In Sharjah, driving without insurance is a punishable offense. The fine for driving without insurance in Sharjah is AED 300 ($90). The fine for the same offense in other Emirates and GCC countries is likely to be different. It is recommended to carry insurance while driving in Sharjah.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the fine for driving without insurance in Sharjah?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b458a403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly UAE government assistant in UAE \n",
      "<|user|>\n",
      "What are the fines for misreporting Emiratisation percentages? \n",
      "<|assistant|>\n",
      "In the UAE, fines are imposed for misreporting Emiratisation percentages as per the UAE Labor Law. The maximum fine for violating this provision is AED 100,000 ($27,000 USD), and the fine is levied by the relevant authorities, including the UAE Ministry of Human Resources and Emiratization. The fines are imposed on individuals, organizations, and legal entities that violate the labor law\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly UAE government assistant in UAE\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What are the fines for misreporting Emiratisation percentages?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=10,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cd34dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c28317aa5ee4bbe82f57290612ee2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d5e22b7c2d4de7a5d4569a3df3fc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/144M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c113093a254f30bcaf0849f02f61ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 6 LFS files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52c51e246ab440cbea8008c22b7f466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bf62a6555b4ed9990aa79f31951f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/72.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3aaf14478f348ca9b083a8858b8947d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42ff7ad629746f1865815892986fe47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/GuhanTofu/TinyLlama-UAE-sft/commit/e14bb2ba0a647bcb6aeaa01654014ad99682a6ad', commit_message='Upload folder using huggingface_hub', commit_description='', oid='e14bb2ba0a647bcb6aeaa01654014ad99682a6ad', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = 'sft-uae/checkpoint-1650'\n",
    "# model_name = 'TinyLlama-UAE-sft'\n",
    "# username = 'GuhanTofu'\n",
    "\n",
    "# from huggingface_hub import HfApi\n",
    "# api = HfApi(token=\"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\")\n",
    "\n",
    "# api.create_repo(\n",
    "#     repo_id = f\"{username}/{model_name}\",\n",
    "#     repo_type=\"model\"\n",
    "# )\n",
    "\n",
    "# api.upload_folder(\n",
    "#     repo_id = f\"{username}/{model_name}\",\n",
    "#     folder_path = model_path\n",
    "# )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b6636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ed8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
